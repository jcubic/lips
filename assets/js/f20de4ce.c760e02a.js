"use strict";(self.webpackChunknew_docs=self.webpackChunknew_docs||[]).push([[3880],{9170:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>o});var s=t(8189),r=t(4848),a=t(8453);const i={slug:"lexer",title:"Internals: State Machine Lexer",authors:"jcubic",image:"/img/lexer.png",tags:["lexer","javascript","internals"]},l=void 0,c={authorsImageUrls:[void 0]},o=[{value:"State Machine Lexer",id:"state-machine-lexer",level:2},{value:"Exmaple",id:"exmaple",level:3},{value:"Syntax Extensions and Constants",id:"syntax-extensions-and-constants",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["The first version of LIPS Scheme had regex based tokenizer. It was using a single regex to split the\ninput string into tokens. In this article I will show the internals of the new\n",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Lexical_analysis",children:"Lexer"})," in LIPS Scheme."]}),"\n",(0,r.jsx)(n.p,{children:"You can still find the first version of what later become LIPS on CodePen:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://codepen.io/jcubic/pen/gvvzdp?editors=0011",children:"Simple Lisp interpreter in JavaScript"})}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["When I started working on version 1.0 (you can read this story in article: ",(0,r.jsx)(n.a,{href:"/blog/lips-history",children:"LIPS Scheme\nHistory"}),"), the code become more and more complex, the regular expression become\ndynamic, mostly because of ",(0,r.jsx)(n.a,{href:"/docs/lips/extension#syntax-extensions",children:"syntax extensions"})," that needed\nto update the regular expression and the tokenizer."]}),"\n",(0,r.jsxs)(n.p,{children:["You can see this code on GitHub on\n",(0,r.jsx)(n.a,{href:"https://github.com/jcubic/lips/blob/old-version/src/lips.js#L201-L204",children:"old-version branch"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:'function makeTokenRe() {\n    var tokens = Object.keys(specials).map(escapeRegex).join(\'|\');\n    return new RegExp(`("(?:\\\\\\\\[\\\\S\\\\s]|[^"])*"|\\\\/(?! )[^\\\\/\\\\\\\\]*(?:\\\\\\\\[\\\\S\\\\s][^\\\\/\\\\\\\\]*)*\\\\/[gimy]*(?=\\\\s|\\\\(|\\\\)|$)|\\\\(|\\\\)|\'|"(?:\\\\\\\\[\\\\S\\\\s]|[^"])+|\\\\n|(?:\\\\\\\\[\\\\S\\\\s]|[^"])*"|;.*|(?:[-+]?(?:(?:\\\\.[0-9]+|[0-9]+\\\\.[0-9]+)(?:[eE][-+]?[0-9]+)?)|[0-9]+\\\\.)[0-9]|\\\\.{2,}|${tokens}|[^(\\\\s)]+)`, \'gim\');\n}\n'})}),"\n",(0,r.jsx)(n.p,{children:"At one point I realized that I need to change my approach into parsing and tokenization,\nbecause you could not add new syntax extensions in the same file that contained the code.\nBecause the whole code was tokenized at once."}),"\n",(0,r.jsx)(n.h2,{id:"state-machine-lexer",children:"State Machine Lexer"}),"\n",(0,r.jsx)(n.p,{children:"The limitation of syntax extension lead into introducing a new Lexer and a Streaming\nParser (if you're interested in this topic I will be writing an article about this in the\nfuture)."}),"\n",(0,r.jsxs)(n.p,{children:["The new Lexer is much simpler and easier to maintain, only had one bug recenly related to\nLexer inner working (",(0,r.jsx)(n.a,{href:"https://github.com/jcubic/lips/issues/433",children:"#433"}),")."]}),"\n",(0,r.jsx)(n.p,{children:"The new Lexer is a class that have rules for the state machine, this is an example\nsequance of rules for a string:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"Lexer.string = Symbol.for('string');\nLexer.string_escape = Symbol.for('string_escape');\n...\nLexer._rules = [\n    ...\n    [/\"/, null, null, Lexer.string, null],\n    [/\"/, null, null, null, Lexer.string],\n    [/\"/, null, null, Lexer.string_escape, Lexer.string],\n    [/\\\\/, null, null, Lexer.string, Lexer.string_escape],\n    [/./, /\\\\/, null, Lexer.string_escape, Lexer.string],\n    ...\n]\n"})}),"\n",(0,r.jsx)(n.p,{children:"The single rule is consisted of a currect character, next character, and a previous\ncharacter (they can be single character strings or regular expressions).  If the character\nis null it can be any character. The last two elements of the array are the starting and\nthe ending state (they are symbols so they are unique values)."}),"\n",(0,r.jsx)(n.p,{children:"The Lexer start with null state and iterate over every rule on every character until it\nfind a match.  If a rule enters a state and the state finish with null it means that the\nrule sequance was matched, and full token is created."}),"\n",(0,r.jsx)(n.p,{children:"If no rules match and the state is not null then the characters is collected and will be\nincluded in final token."}),"\n",(0,r.jsx)(n.p,{children:"That's why in above example there are no rule like this:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"[/./, null, null, Lexer.string, Lexer.string]\n"})}),"\n",(0,r.jsx)(n.p,{children:"This rule may be added in the future to speed up the Lexer."}),"\n",(0,r.jsx)(n.h3,{id:"exmaple",children:"Exmaple"}),"\n",(0,r.jsx)(n.p,{children:"When we have a string like this:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:'"foo\\"bar"\n'})}),"\n",(0,r.jsxs)(n.p,{children:["It matches the second rule because the first character is a quote, so it enters\n",(0,r.jsx)(n.code,{children:"Lexer.string"})," state.  The first rule don't match becuase the inital state is null. For\ncharacters ",(0,r.jsx)(n.code,{children:"foo"})," it collects the tokens becasue no rule match them. When it finds slash\n",(0,r.jsx)(n.code,{children:"\\"})," it changes state from ",(0,r.jsx)(n.code,{children:"Lexer.string"})," to ",(0,r.jsx)(n.code,{children:"Lexer.string_escape"}),", and for next character\nit enters again ",(0,r.jsx)(n.code,{children:"Lexer.string"}),".  Then it consumes sequence of characters ",(0,r.jsx)(n.code,{children:"bar"}),", and the\nlast quote maches the first rule. And that's how we have the full token."]}),"\n",(0,r.jsx)(n.h3,{id:"syntax-extensions-and-constants",children:"Syntax Extensions and Constants"}),"\n",(0,r.jsxs)(n.p,{children:["The static rules are located in ",(0,r.jsx)(n.code,{children:"Lexer._rules"}),", but ",(0,r.jsx)(n.code,{children:"Lexer.rules"})," is a getter that create the final\nrules dynamically by adding all tokens added as syntax extensions (they are called specials in the\ncode). This is also where other constats that starts with hash are added like: ",(0,r.jsx)(n.code,{children:"#t"}),", ",(0,r.jsx)(n.code,{children:"#f"}),", or\n",(0,r.jsx)(n.code,{children:"#void"}),". They are added together with syntax extension to handle the rules matching order."]}),"\n",(0,r.jsxs)(n.p,{children:["The syntax extension create a lexer rule using ",(0,r.jsx)(n.code,{children:"Lexer.literal_rule"})," that creates an array of rules\nthat match literal characters in the token, passed as first character."]}),"\n",(0,r.jsx)(n.p,{children:"Lexer is important not only when it reads input LIPS Scheme code, it's also used when reading from\nI/O ports."}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsxs)(n.p,{children:["And that's it, this is whole Lexer. As you can see reading the above, it's very simple, easy to\nmaintain. If you want to look how it works for yourself. You can jump into\n",(0,r.jsx)(n.a,{href:"https://github.com/jcubic/lips/tree/master/src",children:"the source code"}),".  And\nsearch for ",(0,r.jsx)(n.code,{children:'"class Lexer"'}),", ",(0,r.jsx)(n.code,{children:'"Lexer._rule"'}),", ",(0,r.jsx)(n.code,{children:"Object.defineProperty(Lexer, 'rules'"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"The source code is in one file, so to navigate you need to use search. I've made an attempt to split\nthe code into modules, but failed. Becuse of Rollup errors about circular dependencies."}),"\n",(0,r.jsxs)(n.p,{children:["This was first part of articles about ",(0,r.jsx)(n.a,{href:"https://github.com/jcubic/lips/issues/437",children:"LIPS Scheme Internals"}),"."]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>l});var s=t(6540);const r={},a=s.createContext(r);function i(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(a.Provider,{value:n},e.children)}},8189:e=>{e.exports=JSON.parse('{"permalink":"/blog/lexer","editUrl":"https://github.com/jcubic/lips/tree/master/docs/blog/2025-02-20-lexer.md","source":"@site/blog/2025-02-20-lexer.md","title":"Internals: State Machine Lexer","description":"The first version of LIPS Scheme had regex based tokenizer. It was using a single regex to split the","date":"2025-02-20T00:00:00.000Z","tags":[{"inline":true,"label":"lexer","permalink":"/blog/tags/lexer"},{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"},{"inline":true,"label":"internals","permalink":"/blog/tags/internals"}],"readingTime":3.715,"hasTruncateMarker":true,"authors":[{"name":"Jakub T. Jankiewicz","title":"LIPS maintainer","url":"https://jakub.jankiewicz.org/","imageURL":"https://github.com/jcubic.png","key":"jcubic","page":null}],"frontMatter":{"slug":"lexer","title":"Internals: State Machine Lexer","authors":"jcubic","image":"/img/lexer.png","tags":["lexer","javascript","internals"]},"unlisted":false,"nextItem":{"title":"Scheme Regex literals in Emacs","permalink":"/blog/emacs-scheme-regex"}}')}}]);