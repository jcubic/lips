"use strict";(self.webpackChunknew_docs=self.webpackChunknew_docs||[]).push([[9923],{8401:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});var t=s(3095),a=s(4848),i=s(8453);const o={slug:"syntax-extensions",title:"Internals: Syntax Extensions",authors:"jcubic",image:"/img/syntax-extensions.png",tags:["parser","javascript","internals"]},r=void 0,l={authorsImageUrls:[void 0]},c=[{value:"Usage of syntax extension",id:"usage-of-syntax-extension",level:2},{value:"Implementation",id:"implementation",level:2},{value:"Specials",id:"specials",level:3},{value:"Lexer",id:"lexer",level:3},{value:"Parser",id:"parser",level:3},{value:"Conclusion",id:"conclusion",level:3}];function d(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:"Syntax extensions are a feature in LIPS Scheme that allow users to add new syntax. They work similar\nto readers, macro in Common Lisp. You create a sequence of characters that maps to a function that\nis executed by the parser, the function works similar to a macro and a result of the function is\nreturned by the parser in place of the sequence of defined characters."}),"\n",(0,a.jsx)(n.h2,{id:"usage-of-syntax-extension",children:"Usage of syntax extension"}),"\n",(0,a.jsx)(n.p,{children:"And Example of syntax extension used by LIPS Scheme are gensym literals."}),"\n",(0,a.jsx)(n.p,{children:"If you look at the source code in lib/bootstrap.scm file, you will see this code:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-scheme",children:'(set-special! "#:" \'gensym-interal)\n\n(define (gensym-interal symbol)\n  "(gensym-interal symbol)\n\n   Parser extension that creates a new quoted named gensym."\n  `(quote ,(gensym symbol)))\n'})}),"\n",(0,a.jsx)(n.p,{children:"And when you evaluate:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-scheme",children:"#:foo\n;; ==> #:foo\n"})}),"\n",(0,a.jsx)(n.p,{children:"Which is representation of named gensyms."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-scheme",children:'(gensym "foo")\n;; ==>> #:foo\n(eq? #:foo #:foo)\n;; ==> #f\n'})}),"\n",(0,a.jsx)(n.p,{children:"The syntax extension can be a function or a macro, there is only small difference, so in the future\nthe macro support may be removed."}),"\n",(0,a.jsxs)(n.p,{children:["You can read more about ",(0,a.jsx)(n.a,{href:"/docs/lips/extension#syntax-extensions",children:"syntax extension in documentation"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,a.jsx)(n.h3,{id:"specials",children:"Specials"}),"\n",(0,a.jsxs)(n.p,{children:["In source code, the syntax extensions are called ",(0,a.jsx)(n.code,{children:"specials"})," it was initial name for the quotation symbols like:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"'"})," \u2013 quote"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"`"})," \u2013 quasiquote"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:","})," \u2013 unquote"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:",@"})," \u2013 unquote-splicing"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Later they were extended into a user defined sequence of characters that changed how the parser works."}),"\n",(0,a.jsxs)(n.p,{children:["The first implementation of syntax extension date back into regex based tokenizer. You can read\nabout this in previous blog post about ",(0,a.jsx)(n.a,{href:"/blog/lexer",children:"Finite-State Machine Lexer"}),". The problem with\nold regex based tokenize was that the content of the file needed to be converted to tokens before it\nwas passed to the parser. And because of this the code that added new special sequence, like ",(0,a.jsx)(n.code,{children:'"#:"'}),",\ncould not be part of the same file as the code that used the new syntax."]}),"\n",(0,a.jsx)(n.p,{children:"This led to the creation of a new Lexer and a new parsing approach."}),"\n",(0,a.jsxs)(n.p,{children:["In the current form ",(0,a.jsx)(n.code,{children:"specials"})," is an inline object that implements an Event Emitter:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:"var specials = {\n    LITERAL: Symbol.for('literal'),\n    SPLICE: Symbol.for('splice'),\n    SYMBOL: Symbol.for('symbol'),\n    names: function() {\n        return Object.keys(this.__list__);\n    },\n    type: function(name) {\n        try {\n            return this.get(name).type;\n        } catch(e) {\n            console.log({name});\n            console.log(e);\n            return null;\n        }\n    },\n    get: function(name) {\n        return this.__list__[name];\n    },\n    // events are used in Lexer dynamic rules\n    off: function(name, fn = null) {\n        if (Array.isArray(name)) {\n            name.forEach(name => this.off(name, fn));\n        } else if (fn === null) {\n            delete this.__events__[name];\n        } else {\n            this.__events__ = this.__events__.filter(test => test !== fn);\n        }\n    },\n    on: function(name, fn) {\n        if (Array.isArray(name)) {\n            name.forEach(name => this.on(name, fn));\n        } else if (!this.__events__[name]) {\n            this.__events__[name] = [fn];\n        } else {\n            this.__events__[name].push(fn);\n        }\n    },\n    trigger: function(name, ...args) {\n        if (this.__events__[name]) {\n            this.__events__[name].forEach(fn => fn(...args));\n        }\n    },\n    remove: function(name) {\n        delete this.__list__[name];\n        this.trigger('remove');\n    },\n    append: function(name, value, type) {\n        this.__list__[name] = {\n            seq: name,\n            symbol: value,\n            type\n        };\n        this.trigger('append');\n    },\n    __events__: {},\n    __list__: {}\n};\n"})}),"\n",(0,a.jsxs)(n.p,{children:["The code then adds all built-in specials listed above using ",(0,a.jsx)(n.code,{children:"append"}),", but also added an array of them\nas read only ",(0,a.jsx)(n.code,{children:"__builtins__"}),". You can access specials object from inside LIPS:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-scheme",children:'lips.specials.__builtins__\n;; ==> #("\'" "`" ",@" "," "\'>")\n'})}),"\n",(0,a.jsxs)(n.p,{children:["The list of all specials are saved in ",(0,a.jsx)(n.code,{children:"specials.__list__"})," object."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-scheme",children:'(Object.keys lips.specials.__list__)\n;; ==> #("\'" "`" ",@" "," "\'>" "#:" "&" "#\\"" "~" "\u2019" "#"\n;;       "#u8" "#s8" "#u16" "#s16" "#u32" "#s32" "#f32" "#f64")\n'})}),"\n",(0,a.jsxs)(n.p,{children:["An interesting element on the list is ",(0,a.jsx)(n.code,{children:'"\u2019"'}),". It's an invalid quotation mark used by ",(0,a.jsx)(n.a,{href:"https://standards.scheme.org/unofficial/errata-corrected-r7rs.pdf",children:"official R7RS\nspecification"}),", and probably also\nby some PDF books you can find online. It throws an error when copy and pasted the code from the spec."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-scheme",children:"\u2019(quasiquote (list (unquote (+ 1 2)) 4))\n;; ==> Error: You're using an invalid Unicode quote character.\n;; ==> Run: (set-special! \"\u2019\" 'quote) to allow the use of this type of quote at line 1\n"})}),"\n",(0,a.jsxs)(n.p,{children:["The error suggests to map that sequence to ",(0,a.jsx)(n.code,{children:"quote"}),", so it will act exactly the same as normal\nquotation."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-scheme",children:'(set-special! "\u2019" \'quote)\n\u2019(quasiquote (list (unquote (+ 1 2)) 4))\n;; ==> (quasiquote (list (unquote (+ 1 2)) 4))\n'})}),"\n",(0,a.jsx)(n.p,{children:"The reason why it doesn't just work out of the box and throw an error instead, is so the users know\nthat this is not a valid Scheme syntax, it will make them confused when switching to a different\nScheme implementation."}),"\n",(0,a.jsx)(n.h3,{id:"lexer",children:"Lexer"}),"\n",(0,a.jsxs)(n.p,{children:["All specials are included in dynamic ",(0,a.jsx)(n.code,{children:"Lexer.rules"}),", that are part of its State Machine.  So when you\nadd a new syntax extension, you in the fact modify the Lexer rules and tokens that are created."]}),"\n",(0,a.jsxs)(n.p,{children:["Specials are handled the same way as parser constants like ",(0,a.jsx)(n.code,{children:"#null"})," or ",(0,a.jsx)(n.code,{children:"#void"})," and use static method:\n",(0,a.jsx)(n.code,{children:"Lexer.literal_rule"})," to split individual characters into a state machine rules that will match full\nsequance of characters."]}),"\n",(0,a.jsxs)(n.p,{children:["The lexer caches the dynamic rules in ",(0,a.jsx)(n.code,{children:"Lexer._cache.rules"}),", the cache is invalidated when new syntax\nextension is added. The ",(0,a.jsx)(n.code,{children:"specials.append()"})," method trigger the ",(0,a.jsx)(n.code,{children:"append"})," event that invalidate the\ncache."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:"specials.on(['remove', 'append'], function() {\n    Lexer._cache.valid = false;\n    Lexer._cache.rules = null;\n});\n"})}),"\n",(0,a.jsxs)(n.p,{children:["As you can see, the cache is also invalidated when syntax extension is removed. It happens when user\ncalls ",(0,a.jsx)(n.code,{children:"unset-special!"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-scheme",children:'(set-special! "::" \'foo)\n\n(define (foo x)\n  `(quote ,x))\n\n::foo\n;; ==> foo\n\n(unset-special! "::")\n\n::foo\n;; ==> Unbound variable `::foo\'\n'})}),"\n",(0,a.jsx)(n.h3,{id:"parser",children:"Parser"}),"\n",(0,a.jsxs)(n.p,{children:["When Parser (In ",(0,a.jsx)(n.code,{children:"_read_object"})," private method), get a token (using ",(0,a.jsx)(n.code,{children:"peek()"})," method that calls\n",(0,a.jsx)(n.code,{children:"__lexer__.peek()"}),") that is a special, it checks two cases:"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["if the special is ",(0,a.jsx)(n.code,{children:"builtin"}),", then it just extends it with the long form ",(0,a.jsx)(n.code,{children:"'x"})," become ",(0,a.jsx)(n.code,{children:"(quote x)"}),"."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"if not, it grabs the value of the symbol associated with a given special from environment."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"If the value is a function, it executes that function"}),"\n",(0,a.jsx)(n.li,{children:"If the value is a macro, it evaluates the long form of the syntax extension"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Both cases use ",(0,a.jsx)(n.code,{children:"_with_syntax_scope"})," method that add ",(0,a.jsx)(n.code,{children:"stdin"})," as ",(0,a.jsx)(n.code,{children:"ParserInputPort"})," and extends the\n",(0,a.jsx)(n.code,{children:"lips"})," global to add ",(0,a.jsx)(n.code,{children:"__parser__"})," and point it to the instance of the Parser. This way the syntax\nextension can access functions like ",(0,a.jsx)(n.code,{children:"read"})," or ",(0,a.jsx)(n.code,{children:"read-char"})," or can also manipulate the parser instance."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(n.p,{children:"And this is whole implementation. Syntax extensions in LIPS Scheme are a powerful feature, enabling\nusers to define custom syntax beyond just plain macros. Even that their implementation is not that\nhard to understand, they contain a lot of creative potential, allowing developers to extend the\nlanguage in new ways."})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>r});var t=s(6540);const a={},i=t.createContext(a);function o(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(i.Provider,{value:n},e.children)}},3095:e=>{e.exports=JSON.parse('{"permalink":"/blog/syntax-extensions","editUrl":"https://github.com/jcubic/lips/tree/master/docs/blog/2025-04-21-syntax-extensions.md","source":"@site/blog/2025-04-21-syntax-extensions.md","title":"Internals: Syntax Extensions","description":"Syntax extensions are a feature in LIPS Scheme that allow users to add new syntax. They work similar","date":"2025-04-21T00:00:00.000Z","tags":[{"inline":true,"label":"parser","permalink":"/blog/tags/parser"},{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"},{"inline":true,"label":"internals","permalink":"/blog/tags/internals"}],"readingTime":5.395,"hasTruncateMarker":true,"authors":[{"name":"Jakub T. Jankiewicz","title":"LIPS maintainer","url":"https://jakub.jankiewicz.org/","imageURL":"https://github.com/jcubic.png","key":"jcubic","page":null}],"frontMatter":{"slug":"syntax-extensions","title":"Internals: Syntax Extensions","authors":"jcubic","image":"/img/syntax-extensions.png","tags":["parser","javascript","internals"]},"unlisted":false,"nextItem":{"title":"Internals: Finite-State Machine Lexer","permalink":"/blog/lexer"}}')}}]);