"use strict";(self.webpackChunknew_docs=self.webpackChunknew_docs||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"serialization","metadata":{"permalink":"/blog/serialization","editUrl":"https://github.com/jcubic/lips/tree/master/docs/blog/2025-09-14-object-serialization.md","source":"@site/blog/2025-09-14-object-serialization.md","title":"How to serialize any object in JavaScript?","description":"In this article, I will explain how the serialization of objects (in dump compiler) works in LIPS.","date":"2025-09-14T00:00:00.000Z","tags":[{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"},{"inline":true,"label":"internals","permalink":"/blog/tags/internals"}],"readingTime":4.88,"hasTruncateMarker":true,"authors":[{"name":"Jakub T. Jankiewicz","title":"LIPS maintainer","url":"https://jakub.jankiewicz.org/","imageURL":"https://github.com/jcubic.png","key":"jcubic","page":null}],"frontMatter":{"slug":"serialization","title":"How to serialize any object in JavaScript?","authors":"jcubic","image":"/img/object-serialization.png","tags":["javascript","internals"]},"unlisted":false,"nextItem":{"title":"Internals: Syntax Extensions","permalink":"/blog/syntax-extensions"}},"content":"In this article, I will explain how the serialization of objects (in dump compiler) works in LIPS.\\n\\n\x3c!-- truncate --\x3e\\n\\nAs a way of optimization, I decided to save the representation of the parsed code as a file.\\n\\nThe input Scheme code was read, parsed, and the Tree structure was then serialized. The code had\\nobjects, like a Pair, that needed to be saved and restored to and from a file.\\n\\n## Serialization with JSON\\n\\nThe first approach was to use JSON and take the benefit of the second argument in\\nJSON.stringify and JSON.parse.\\n\\nEach object was saved as `{\\"@\\": \\"class\\", \\"#\\": data}`.\\n\\nSo the Pair class was saved as:\\n\\n```json\\n{\\"@\\": \\"Pair\\", \\"#\\": [car, cdr]}\\n```\\n\\nLater, the class field was change into an index to make the output JSON file smaller.\\n\\nThe code responsible for serialization is still part of the project, so LIPS can still open old files. It looks like this:\\n\\n```javascript\\nfunction serialize(data) {\\n    return JSON.stringify(data, function(key, value) {\\n        const v0 = this[key];\\n        if (v0) {\\n            if (v0 instanceof RegExp) {\\n                return {\\n                    \'@\': mangle_name(\'regex\'),\\n                    \'#\': [v0.source, v0.flags]\\n                };\\n            }\\n            var cls = mangle_name(v0.constructor.__class__);\\n            if (!is_undef(cls)) {\\n                return {\\n                    \'@\': cls,\\n                    \'#\': v0.serialize()\\n                };\\n            }\\n        }\\n        return value;\\n    });\\n}\\n```\\n\\nAs you can see from the code, it can serialize the regular expressions and internal classes by calling a serialize method.\\n\\nHere is example serialize method for class Pair:\\n\\n```javascript\\nPair.prototype.serialize = function() {\\n    return [\\n        this.car,\\n        this.cdr\\n    ];\\n};\\n```\\n\\nFunction `mangle_name` was also an optimization that make the classes smaller. Each class had an\\nindex where Pair had index `0` saved in `serialization_map`:\\n\\n```javascript\\nvar serialization_map = {\\n    \'pair\': ([car, cdr]) => Pair(car, cdr),\\n    \'number\': function(value) {\\n        if (LString.isString(value)) {\\n            return LNumber([value, 10]);\\n        }\\n        return LNumber(value);\\n    },\\n    \'regex\': function([pattern, flag]) {\\n        return new RegExp(pattern, flag);\\n    },\\n    \'nil\': function() {\\n        return nil;\\n    },\\n    \'symbol\': function(value) {\\n        if (LString.isString(value)) {\\n            return LSymbol(value);\\n        } else if (Array.isArray(value)) {\\n            return LSymbol(Symbol.for(value[0]));\\n        }\\n    },\\n    \'string\': LString,\\n    \'character\': LCharacter\\n};\\n```\\n\\nA serialization map was also used to unserialized the data from the JSON. Here is how the function look like:\\n\\n```javascript\\nfunction unserialize(string) {\\n    return JSON.parse(string, (_, object) => {\\n        if (object && typeof object === \'object\') {\\n            if (!is_undef(object[\'@\'])) {\\n                var cls = resolve_name(object[\'@\']);\\n                if (serialization_map[cls]) {\\n                    return serialization_map[cls](object[\'#\']);\\n                }\\n            }\\n        }\\n        return object;\\n    });\\n}\\n```\\n\\nFunction `resolve_name` is reverse of `mangle_name` which return class name based on index.\\n\\n## Serialization with compressed CBOR\\n\\nBut the output of JSON file was big, so I decided to create a better serialization. This is when I\\nfound about [CBOR binary data format](https://en.wikipedia.org/wiki/CBOR). I found a fast library\\n[cbor-x](https://github.com/kriszyp/cbor-x).\\n\\nThe library has an API that allow adding custom extensions to add any type of objects.\\n\\nThis the function that add support for all internal data types:\\n\\n```javascript\\nconst cbor = (function() {\\n\\n    var types = {\\n        \'pair\': Pair,\\n        \'symbol\': LSymbol,\\n        \'number\': LNumber,\\n        \'string\': LString,\\n        \'character\': LCharacter,\\n        \'nil\': nil.constructor,\\n        \'regex\': RegExp\\n    };\\n\\n    function serializer(Class, fn) {\\n        return {\\n            deserialize: fn,\\n            Class\\n        };\\n    }\\n\\n    var encoder = new Encoder();\\n\\n    const cbor_serialization_map = {};\\n    for (const [ name, fn ] of Object.entries(serialization_map)) {\\n        const Class = types[name];\\n        cbor_serialization_map[name] = serializer(Class, fn);\\n    }\\n    // add CBOR data mapping\\n    let tag = 43311;\\n    Object.keys(cbor_serialization_map).forEach(type => {\\n        const data = cbor_serialization_map[type];\\n        if (typeof data === \'function\') {\\n            const Class = data;\\n            addExtension({\\n                Class,\\n                tag,\\n                encode(instance, encode) {\\n                    encode(instance.serialize());\\n                },\\n                decode(data) {\\n                    return new Class(data);\\n                }\\n            });\\n        } else {\\n            const { deserialize, Class } = data;\\n            addExtension({\\n                Class,\\n                tag,\\n                encode(instance, encode) {\\n                    if (instance instanceof RegExp) {\\n                        return encode([instance.source, instance.flags]);\\n                    }\\n                    encode(instance.serialize());\\n                },\\n                decode(data) {\\n                    return deserialize(data);\\n                }\\n            });\\n        }\\n        tag++;\\n    });\\n    return encoder;\\n})();\\n```\\n\\nAfter this, you only need to use the `cbor.encode` to serialize the data and `cbor.decode` to unserialize.\\n\\nBut the CBOR output data was still big, it could be compressed (I could also do the same with JSON files).\\n\\nI was searching for good and fast compression library and found implementation of\\n[LZJB](https://en.wikipedia.org/wiki/Jeff_Bonwick#LZJB) and a library [lzjb-k](https://github.com/copy/jslzjb-k)\\non GitHub. I converted it into a module and published to NPM as [lzjb-pack](https://www.npmjs.com/package/lzjb-pack).\\n\\nTo create a binary file I compressed the output CBOR data and add a magic number in front.\\n\\n```javascript\\nimport { addExtension, Encoder } from \'cbor-x\';\\n\\nfunction encode_magic() {\\n    const VERSION = 1;\\n    const encoder = new TextEncoder(\'utf-8\');\\n    return encoder.encode(`LIPS${VERSION.toString().padStart(3, \' \')}`);\\n}\\n\\nconst MAGIC_LENGTH = 7;\\n\\nfunction decode_magic(obj) {\\n    const decoder = new TextDecoder(\'utf-8\');\\n    const prefix = decoder.decode(obj.slice(0, MAGIC_LENGTH));\\n    const name = prefix.substring(0, 4);\\n    if (name === \'LIPS\') {\\n        const m = prefix.match(/^(....).*([0-9]+)$/);\\n        if (m) {\\n            return {\\n                type: m[1],\\n                version: Number(m[2])\\n            };\\n        }\\n    }\\n    return {\\n        type: \'unknown\'\\n    };\\n}\\n```\\n\\nThis will make it easier for the future to read multiple version of the binary data.\\n\\nThe serialization and deserialization functions look like this:\\n\\n```javascript\\nimport { pack, unpack } from \'lzjb-pack\';\\n\\nfunction serialize_bin(obj) {\\n    const magic = encode_magic();\\n    const payload = cbor.encode(obj);\\n    return merge_uint8_array(magic, pack(payload, { magic: false }));\\n}\\n\\nfunction unserialize_bin(data) {\\n    const { type, version } = decode_magic(data);\\n    if (type === \'LIPS\' && version === 1) {\\n        const arr = unpack(data.slice(MAGIC_LENGTH), { magic: false });\\n        return cbor.decode(arr);\\n    } else {\\n        throw new Error(`Invalid file format ${type}`);\\n    }\\n}\\n```\\n\\nAnd this is the whole serialization, done by LIPS.\\n\\n## Conclusion\\n\\nThe compressed CBOR file is way smaller than the source code and way smaller than JSON file,\\nthat is way bigger then the input file. But the JSON can still be compressed which was not tested.\\n\\nThis is summary of the size of the standard library serialized in this way:\\n\\n| File Comparison    | Size |Size Difference | Percentage Change | Description |\\n|--------------------|------|----------------|-------------------|-------------|\\n| std.scm            | 207k | -              | -                 | source code |\\n| std.xcm            | 478K | +271k          | +130.92%          | JSON        |\\n| std.xcb            | 105K | -102k          | -49.28%           | CBOR+LZJB   |"},{"id":"syntax-extensions","metadata":{"permalink":"/blog/syntax-extensions","editUrl":"https://github.com/jcubic/lips/tree/master/docs/blog/2025-04-21-syntax-extensions.md","source":"@site/blog/2025-04-21-syntax-extensions.md","title":"Internals: Syntax Extensions","description":"Syntax extensions are a feature in LIPS Scheme that allow users to add new syntax. They work similar","date":"2025-04-21T00:00:00.000Z","tags":[{"inline":true,"label":"parser","permalink":"/blog/tags/parser"},{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"},{"inline":true,"label":"internals","permalink":"/blog/tags/internals"}],"readingTime":5.5,"hasTruncateMarker":true,"authors":[{"name":"Jakub T. Jankiewicz","title":"LIPS maintainer","url":"https://jakub.jankiewicz.org/","imageURL":"https://github.com/jcubic.png","key":"jcubic","page":null}],"frontMatter":{"slug":"syntax-extensions","title":"Internals: Syntax Extensions","authors":"jcubic","image":"/img/syntax-extensions.png","tags":["parser","javascript","internals"]},"unlisted":false,"prevItem":{"title":"How to serialize any object in JavaScript?","permalink":"/blog/serialization"},"nextItem":{"title":"Internals: Finite-State Machine Lexer","permalink":"/blog/lexer"}},"content":"Syntax extensions are a feature in LIPS Scheme that allow users to add new syntax. They work similar\\nto readers, macro in Common Lisp. You create a sequence of characters that maps to a function that\\nis executed by the parser, the function works similar to a macro and a result of the function is\\nreturned by the parser in place of the sequence of defined characters.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Usage of syntax extension\\n\\nAnd Example of syntax extension used by LIPS Scheme are gensym literals.\\n\\nIf you look at the source code in lib/bootstrap.scm file, you will see this code:\\n\\n```scheme\\n;; the name should probably be gensym-literal\\n(set-special! \\"#:\\" \'gensym-interal)\\n\\n(define (gensym-interal symbol)\\n  \\"(gensym-interal symbol)\\n\\n   Parser extension that creates a new quoted named gensym.\\"\\n  `(quote ,(gensym symbol)))\\n```\\n\\nAnd when you evaluate:\\n```scheme\\n#:foo\\n;; ==> #:foo\\n```\\n\\nWhich is representation of named gensyms.\\n\\n```scheme\\n(gensym \\"foo\\")\\n;; ==>> #:foo\\n(eq? #:foo #:foo)\\n;; ==> #f\\n```\\n\\nThe syntax extension can be a function or a macro, there is only small difference, so in the future\\nthe macro support may be removed.\\n\\nYou can read more about [syntax extension in documentation](/docs/lips/extension#syntax-extensions).\\n\\n## Implementation\\n\\n### Specials\\n\\nIn source code, the syntax extensions are called `specials` it was initial name for the quotation symbols like:\\n\\n* `\'` \u2013 quote\\n* `` ` `` \u2013 quasiquote\\n* `` , `` \u2013 unquote\\n* `,@` \u2013 unquote-splicing\\n\\nLater they were extended into a user defined sequence of characters that changed how the parser works.\\n\\nThe first implementation of syntax extension date back into regex based tokenizer. You can read\\nabout this in previous blog post about [Finite-State Machine Lexer](/blog/lexer). The problem with\\nold regex based tokenize was that the content of the file needed to be converted to tokens before it\\nwas passed to the parser. And because of this the code that added new special sequence, like `\\"#:\\"`,\\ncould not be part of the same file as the code that used the new syntax.\\n\\nThis led to the creation of a new Lexer and a new parsing approach.\\n\\nIn the current form `specials` is an inline object that implements an Event Emitter:\\n\\n```javascript\\n// var is a lefover from the time when the codebase was only ES5\\nvar specials = {\\n    LITERAL: Symbol.for(\'literal\'),\\n    SPLICE: Symbol.for(\'splice\'),\\n    SYMBOL: Symbol.for(\'symbol\'),\\n    names: function() {\\n        return Object.keys(this.__list__);\\n    },\\n    type: function(name) {\\n        try {\\n            return this.get(name).type;\\n        } catch(e) {\\n            console.log({name});\\n            console.log(e);\\n            return null;\\n        }\\n    },\\n    get: function(name) {\\n        return this.__list__[name];\\n    },\\n    // events are used in Lexer dynamic rules\\n    off: function(name, fn = null) {\\n        if (Array.isArray(name)) {\\n            name.forEach(name => this.off(name, fn));\\n        } else if (fn === null) {\\n            delete this.__events__[name];\\n        } else {\\n            this.__events__ = this.__events__.filter(test => test !== fn);\\n        }\\n    },\\n    on: function(name, fn) {\\n        if (Array.isArray(name)) {\\n            name.forEach(name => this.on(name, fn));\\n        } else if (!this.__events__[name]) {\\n            this.__events__[name] = [fn];\\n        } else {\\n            this.__events__[name].push(fn);\\n        }\\n    },\\n    trigger: function(name, ...args) {\\n        if (this.__events__[name]) {\\n            this.__events__[name].forEach(fn => fn(...args));\\n        }\\n    },\\n    remove: function(name) {\\n        delete this.__list__[name];\\n        this.trigger(\'remove\');\\n    },\\n    append: function(name, value, type) {\\n        this.__list__[name] = {\\n            seq: name,\\n            symbol: value,\\n            type\\n        };\\n        this.trigger(\'append\');\\n    },\\n    __events__: {},\\n    __list__: {}\\n};\\n```\\n\\nThe code then adds all built-in specials listed above using `append`, but also added an array of them\\nas read only `__builtins__`. You can access specials object from inside LIPS:\\n\\n```scheme\\nlips.specials.__builtins__\\n;; ==> #(\\"\'\\" \\"`\\" \\",@\\" \\",\\" \\"\'>\\")\\n```\\n\\nThe list of all specials are saved in `specials.__list__` object.\\n\\n```scheme\\n(Object.keys lips.specials.__list__)\\n;; ==> #(\\"\'\\" \\"`\\" \\",@\\" \\",\\" \\"\'>\\" \\"#:\\" \\"&\\" \\"#\\\\\\"\\" \\"~\\" \\"\u2019\\" \\"#\\"\\n;;       \\"#u8\\" \\"#s8\\" \\"#u16\\" \\"#s16\\" \\"#u32\\" \\"#s32\\" \\"#f32\\" \\"#f64\\")\\n```\\n\\nAn interesting element on the list is `\\"\u2019\\"`. It\'s an invalid quotation mark used by [official R7RS\\nspecification](https://standards.scheme.org/unofficial/errata-corrected-r7rs.pdf), and probably also\\nby some PDF books you can find online. It throws an error when copy and pasted the code from the spec.\\n\\n```scheme\\n\u2019(quasiquote (list (unquote (+ 1 2)) 4))\\n;; ==> Error: You\'re using an invalid Unicode quote character.\\n;; ==> Run: (set-special! \\"\u2019\\" \'quote) to allow the use of this type of quote at line 1\\n```\\n\\nThe error suggests to map that sequence to `quote`, so it will act exactly the same as normal\\nquotation.\\n\\n```scheme\\n(set-special! \\"\u2019\\" \'quote)\\n\u2019(quasiquote (list (unquote (+ 1 2)) 4))\\n;; ==> (quasiquote (list (unquote (+ 1 2)) 4))\\n```\\n\\nThe reason why it doesn\'t just work out of the box and throw an error instead, is so the users know\\nthat this is not a valid Scheme syntax, it will make them confused when switching to a different\\nScheme implementation.\\n\\n### Lexer\\n\\nAll specials are included in dynamic `Lexer.rules`, that are part of its State Machine.  So when you\\nadd a new syntax extension, you in the fact modify the Lexer rules and tokens that are created.\\n\\nSpecials are handled the same way as parser constants like `#null` or `#void` and use static method:\\n`Lexer.literal_rule` to split individual characters into a state machine rules that will match full\\nsequance of characters.\\n\\nThe lexer caches the dynamic rules in `Lexer._cache.rules`, the cache is invalidated when new syntax\\nextension is added. The `specials.append()` method trigger the `append` event that invalidate the\\ncache.\\n\\n```javascript\\nspecials.on([\'remove\', \'append\'], function() {\\n    Lexer._cache.valid = false;\\n    Lexer._cache.rules = null;\\n});\\n```\\n\\nAs you can see, the cache is also invalidated when syntax extension is removed. It happens when user\\ncalls `unset-special!`.\\n\\n```scheme\\n(set-special! \\"::\\" \'foo)\\n\\n(define (foo x)\\n  `(quote ,x))\\n\\n::foo\\n;; ==> foo\\n\\n(unset-special! \\"::\\")\\n\\n::foo\\n;; ==> Unbound variable `::foo\'\\n```\\n\\n### Parser\\n\\nWhen Parser (In `_read_object` private method), get a token (using `peek()` method that calls\\n`__lexer__.peek()`) that is a special, it checks two cases:\\n\\n1. if the special is `builtin`, then it just extends it with the long form `\'x` become `(quote x)`.\\n2. if not, it grabs the value of the symbol associated with a given special from environment.\\n\\n   * If the value is a function, it executes that function\\n   * If the value is a macro, it evaluates the long form of the syntax extension\\n\\n   Both cases use `_with_syntax_scope` method that add `stdin` as `ParserInputPort` and extends the\\n   `lips` global to add `__parser__` and point it to the instance of the Parser. This way the syntax\\n   extension can access functions like `read` or `read-char` or can also manipulate the parser instance.\\n\\n### Conclusion\\n\\nAnd this is whole implementation. Syntax extensions in LIPS Scheme are a powerful feature, enabling\\nusers to define custom syntax beyond just plain macros. Even that their implementation is not that\\nhard to understand, they contain a lot of creative potential, allowing developers to extend the\\nlanguage in new ways."},{"id":"lexer","metadata":{"permalink":"/blog/lexer","editUrl":"https://github.com/jcubic/lips/tree/master/docs/blog/2025-02-20-lexer.md","source":"@site/blog/2025-02-20-lexer.md","title":"Internals: Finite-State Machine Lexer","description":"The first version of LIPS Scheme had regex based tokenizer. It was using a single regex to split the","date":"2025-02-20T00:00:00.000Z","tags":[{"inline":true,"label":"lexer","permalink":"/blog/tags/lexer"},{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"},{"inline":true,"label":"internals","permalink":"/blog/tags/internals"}],"readingTime":3.865,"hasTruncateMarker":true,"authors":[{"name":"Jakub T. Jankiewicz","title":"LIPS maintainer","url":"https://jakub.jankiewicz.org/","imageURL":"https://github.com/jcubic.png","key":"jcubic","page":null}],"frontMatter":{"slug":"lexer","title":"Internals: Finite-State Machine Lexer","authors":"jcubic","image":"/img/lexer.png","tags":["lexer","javascript","internals"]},"unlisted":false,"prevItem":{"title":"Internals: Syntax Extensions","permalink":"/blog/syntax-extensions"},"nextItem":{"title":"Scheme Regex literals in Emacs","permalink":"/blog/emacs-scheme-regex"}},"content":"The first version of LIPS Scheme had regex based tokenizer. It was using a single regex to split the\\ninput string into tokens. In this article, I will show the internals of the new\\n[Lexer](https://en.wikipedia.org/wiki/Lexical_analysis) in LIPS Scheme.\\n\\n\x3c!-- truncate --\x3e\\n\\nYou can still find the first version of what later become LIPS on CodePen:\\n\\n* [Simple Lisp interpreter in JavaScript](https://codepen.io/jcubic/pen/gvvzdp?editors=0011)\\n\\nWhen I started working on version 1.0 (you can read this story in article: [LIPS Scheme\\nHistory](/blog/lips-history)), the code become more and more complex, the regular expression become\\ndynamic, mostly because of [syntax extensions](/docs/lips/extension#syntax-extensions) that needed\\nto update the regular expression and the tokenizer.\\n\\nYou can see this code on GitHub on\\n[old-version branch](https://github.com/jcubic/lips/blob/old-version/src/lips.js#L201-L204)\\n\\n```javascript\\nfunction makeTokenRe() {\\n    var tokens = Object.keys(specials).map(escapeRegex).join(\'|\');\\n    return new RegExp(`(\\"(?:\\\\\\\\\\\\\\\\[\\\\\\\\S\\\\\\\\s]|[^\\"])*\\"|\\\\\\\\/(?! )[^\\\\\\\\/\\\\\\\\\\\\\\\\]*(?:\\\\\\\\\\\\\\\\[\\\\\\\\S\\\\\\\\s][^\\\\\\\\/\\\\\\\\\\\\\\\\]*)*\\\\\\\\/[gimy]*(?=\\\\\\\\s|\\\\\\\\(|\\\\\\\\)|$)|\\\\\\\\(|\\\\\\\\)|\'|\\"(?:\\\\\\\\\\\\\\\\[\\\\\\\\S\\\\\\\\s]|[^\\"])+|\\\\\\\\n|(?:\\\\\\\\\\\\\\\\[\\\\\\\\S\\\\\\\\s]|[^\\"])*\\"|;.*|(?:[-+]?(?:(?:\\\\\\\\.[0-9]+|[0-9]+\\\\\\\\.[0-9]+)(?:[eE][-+]?[0-9]+)?)|[0-9]+\\\\\\\\.)[0-9]|\\\\\\\\.{2,}|${tokens}|[^(\\\\\\\\s)]+)`, \'gim\');\\n}\\n```\\n\\nAt one point, I realized that I need to change my approach into parsing and tokenization,\\nbecause you could not add new syntax extensions in the same file that contained the code.\\nBecause the whole code was tokenized at once.\\n\\n## Finite State Machine Lexer\\n\\nThe limitation of syntax extension lead into introducing a new Lexer and a Streaming\\nParser (if you\'re interested in this topic, I will be writing an article about this in the\\nfuture).\\n\\nThe new Lexer is much simpler and easier to maintain, only had one bug recently related to\\nLexer inner working ([#433](https://github.com/jcubic/lips/issues/433)).\\n\\nThe new Lexer is a class that have rules for the state machine\\n([FSM](https://en.wikipedia.org/wiki/Finite-state_machine)), this is an example sequance\\nof rules for a string:\\n\\n```javascript\\nLexer.string = Symbol.for(\'string\');\\nLexer.string_escape = Symbol.for(\'string_escape\');\\n...\\nLexer._rules = [\\n    ...\\n    [/\\"/, null, null, Lexer.string, null],\\n    [/\\"/, null, null, null, Lexer.string],\\n    [/\\"/, null, null, Lexer.string_escape, Lexer.string],\\n    [/\\\\\\\\/, null, null, Lexer.string, Lexer.string_escape],\\n    [/./, /\\\\\\\\/, null, Lexer.string_escape, Lexer.string],\\n    ...\\n]\\n```\\n\\nThe single rule is consisted of a current character, next character, and a previous\\ncharacter (they can be single character strings or regular expressions).  If the character\\nis null, it can be any character. The last two elements of the array are the starting and\\nthe ending state (they are symbols, so they are unique values).\\n\\nThe Lexer start with null state and iterate over every rule on every character until it\\nfind a match.  If a rule enters a state and the state finish with null, it means that the\\nrule sequance was matched, and a full token is created.\\n\\nIf no rules match and the state is not null, then the characters are collected and will be\\nincluded in a final token.\\n\\nThat\'s why in above example there are no rule like this:\\n\\n```javascript\\n[/./, null, null, Lexer.string, Lexer.string]\\n```\\n\\nThis rule may be added in the future to speed up the Lexer.\\n\\n### Example\\n\\nWhen we have a string like this:\\n\\n```javascript\\n\\"foo\\\\\\"bar\\"\\n```\\n\\nIt matches the second rule because the first character is a quote, so it enters\\n`Lexer.string` state.  The first rule doesn\'t match because the initial state is null. For\\ncharacters `foo` it collects the tokens because no rule match them. When it finds slash\\n`\\\\` it changes state from `Lexer.string` to `Lexer.string_escape`, and for the next character\\nit enters again `Lexer.string`.  Then it consumes a sequence of characters `bar`, and the\\nlast quote matches the first rule. And that\'s how we have the full token.\\n\\n### Syntax Extensions and Constants\\n\\nThe static rules are located in `Lexer._rules`, but `Lexer.rules` is a getter that create the final\\nrules dynamically by adding all tokens added as syntax extensions (they are called specials in the\\ncode). This is also where other constats that starts with hash are added like: `#t`, `#f`, or\\n`#void`. They are added together with syntax extension to handle the rules matching order.\\n\\nAs an optimization, the value of dynamic rules is cached, and the cache is invalidated when a new\\nsyntax extension is added.\\n\\nThe syntax extension create a lexer rule using `Lexer.literal_rule` that creates an array of rules\\nthat match literal characters in the token, passed as first character.\\n\\nLexer is important not only when it reads input LIPS Scheme code, it\'s also used when reading from\\nI/O ports.\\n\\n## Conclusion\\n\\nAnd that\'s it, this is the whole Lexer. As you can see reading the above, it\'s very simple, easy to\\nmaintain. If you want to look how it works for yourself. You can jump into [the source\\ncode](https://github.com/jcubic/lips/tree/master/src).  And search for `\\"class Lexer\\"`,\\n`\\"Lexer._rule\\"`, `Object.defineProperty(Lexer, \'rules\'`.\\n\\nThe source code is in one file, so to navigate you need to use search. I\'ve made an attempt to split\\nthe code into modules, but failed. Because of Rollup errors about circular dependencies.\\n\\nThis was the first part of articles about [LIPS Scheme\\nInternals](https://github.com/jcubic/lips/issues/437)."},{"id":"emacs-scheme-regex","metadata":{"permalink":"/blog/emacs-scheme-regex","editUrl":"https://github.com/jcubic/lips/tree/master/docs/blog/2024-03-09-emacs-regex-literals.md","source":"@site/blog/2024-03-09-emacs-regex-literals.md","title":"Scheme Regex literals in Emacs","description":"LIPS Scheme support regular expression literals, but it\'s not the only one implementation that","date":"2024-03-09T00:00:00.000Z","tags":[{"inline":true,"label":"scheme","permalink":"/blog/tags/scheme"},{"inline":true,"label":"emacs","permalink":"/blog/tags/emacs"}],"readingTime":2.27,"hasTruncateMarker":true,"authors":[{"name":"Jakub T. Jankiewicz","title":"LIPS maintainer","url":"https://jakub.jankiewicz.org/","imageURL":"https://github.com/jcubic.png","key":"jcubic","page":null}],"frontMatter":{"slug":"emacs-scheme-regex","title":"Scheme Regex literals in Emacs","authors":"jcubic","image":"/img/emacs-scheme-regex.png","tags":["scheme","emacs"]},"unlisted":false,"prevItem":{"title":"Internals: Finite-State Machine Lexer","permalink":"/blog/lexer"},"nextItem":{"title":"LIPS Scheme History","permalink":"/blog/lips-history"}},"content":"LIPS Scheme support regular expression literals, but it\'s not the only one implementation that\\nsupport those.  Other implementation includes [Gauche](https://practical-scheme.net/gauche/) and\\n[Kawa](https://www.gnu.org/software/kawa/index.html).\\n\\nUnfortunetlly, you can\'t easily use those regular expressions in [GNU\\nEmacs](https://en.wikipedia.org/wiki/GNU_Emacs), my main editor of choice.\\n\\n\x3c!--truncate--\x3e\\n\\n## The problem\\n\\nThe main problem is when using vertical bar character inside Scheme code in Emacs. GNU Emacs thinks\\nthat the vertical bar is part of the [symbol](/docs/scheme-intro/data-types#symbols):\\n\\n```scheme\\n(let ((str \\"foo bar\\")\\n      (re #/foo|bar/)) ;; | ))\\n  (str.match re))\\n;; ==> #(\\"foo\\")\\n```\\n\\nThis blog (the same as whole website) use modified PrismJS Scheme mode that supports regular\\nexpressions. But in GNU Emacs there was a need to add `|` after a comment and close the lists that\\nwere ignored by Emacs scheme mode (because they were inside symbol).\\n\\n## The solution\\n\\nI asked a [question on emacs-devel mailing\\nlist](https://lists.gnu.org/archive/html/emacs-devel/2024-02/msg00896.html), on how to solve this\\nproblem. I didn\'t get any reply for days, then suddenly someone [reply with this emacs lisp code\\nsnippet](https://lists.gnu.org/archive/html/emacs-devel/2024-03/msg00282.html).\\n\\n```lisp\\n(defun scheme-regex-patch ()\\n  (setq-local\\n   syntax-propertize-function\\n   (lambda (start end)\\n     (goto-char start)\\n     (funcall\\n      (syntax-propertize-rules\\n       ;; For #/regexp/ syntax\\n       (\\"\\\\\\\\(#\\\\\\\\)/\\\\\\\\(\\\\\\\\\\\\\\\\/\\\\\\\\|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|.\\\\\\\\)*?\\\\\\\\(/\\\\\\\\)\\"\\n        (1 \\"|\\")\\n        (3 \\"|\\"))\\n       ;; For #; comment syntax\\n       (\\"\\\\\\\\(#\\\\\\\\);\\"\\n        (1 (prog1 \\"< cn\\"\\n             (scheme-syntax-propertize-sexp-comment\\n              (point) end)))))\\n      (point) end))))\\n\\n(add-hook \'scheme-mode-hook \'scheme-regex-patch)\\n```\\n\\nThis solution worked great, until I\'ve found that it don\'t properly handle Scheme expression\\ncomments `#;`, that are part of the solution. In the meantime on the mailing list there was discussion\\nabout this feature (probably because it\'s part of GNU Kawa) to integrate with builtin `scheme.el`.\\nSo soon you may not need a hack like this when working with regular expressions.\\n\\nThis is a proposed solution after I said that the code doesn\'t work for Scheme expression comments.\\n\\n```lisp\\n(defun scheme-regex-patch ()\\n   (setq-local\\n    syntax-propertize-function\\n    (lambda (beg end)\\n      (goto-char beg)\\n      (scheme-syntax-propertize-sexp-comment2 end)\\n      (scheme-syntax-propertize-regexp end)\\n      (funcall\\n       (syntax-propertize-rules\\n        (\\"\\\\\\\\(#\\\\\\\\);\\" (1 (prog1 \\"< cn\\"\\n                         (scheme-syntax-propertize-sexp-comment2 end))))\\n        (\\"\\\\\\\\(#\\\\\\\\)/\\" (1 (when (null (nth 8 (save-excursion\\n                                            (syntax-ppss\\n                                             (match-beginning 0)))))\\n                         (put-text-property\\n                          (match-beginning 1)\\n                          (match-end 1)\\n                          \'syntax-table (string-to-syntax \\"|\\"))\\n                         (scheme-syntax-propertize-regexp end)\\n                         nil)\\n                       )))\\n       (point) end))))\\n\\n(defun scheme-syntax-propertize-sexp-comment2 (end)\\n  (let ((state (syntax-ppss)))\\n    (when (eq 2 (nth 7 state))\\n      ;; It\'s a sexp-comment.  Tell parse-partial-sexp where it ends.\\n      (condition-case nil\\n          (progn\\n            (goto-char (+ 2 (nth 8 state)))\\n            ;; FIXME: this doesn\'t handle the case where the sexp\\n            ;; itself contains a #; comment.\\n            (forward-sexp 1)\\n            (put-text-property (1- (point)) (point)\\n                               \'syntax-table (string-to-syntax \\"> cn\\")))\\n        (scan-error (goto-char end))))))\\n\\n(defun scheme-syntax-propertize-regexp (end)\\n  (let* ((state (syntax-ppss))\\n         (within-str (nth 3 state))\\n         (start-delim-pos (nth 8 state)))\\n    (when (and within-str\\n               (char-equal ?# (char-after start-delim-pos)))\\n      (while\\n          (and\\n           (re-search-forward \\"/\\" end \'move)\\n           (eq -1\\n               (% (save-excursion\\n                    (backward-char)\\n                    (skip-chars-backward \\"\\\\\\\\\\\\\\\\\\")) 2))))\\n      (when (< (point) end)\\n        (progn\\n          (put-text-property\\n           (match-beginning 0)\\n           (match-end 0)\\n           \'syntax-table (string-to-syntax \\"|\\")))))))\\n\\n(add-hook \'scheme-mode-hook \'scheme-regex-patch)\\n```\\n\\nYou can read the whole discussion on [emacs-devel mailing list archive](https://lists.gnu.org/archive/html/emacs-devel/2024-03/msg00590.html)."},{"id":"lips-history","metadata":{"permalink":"/blog/lips-history","editUrl":"https://github.com/jcubic/lips/tree/master/docs/blog/2024-03-03-lips-history.md","source":"@site/blog/2024-03-03-lips-history.md","title":"LIPS Scheme History","description":"This is the first article on LIPS blog. In this article I will write about the history of LIPS","date":"2024-03-03T00:00:00.000Z","tags":[{"inline":true,"label":"lips","permalink":"/blog/tags/lips"},{"inline":true,"label":"scheme","permalink":"/blog/tags/scheme"},{"inline":true,"label":"history","permalink":"/blog/tags/history"}],"readingTime":1.825,"hasTruncateMarker":true,"authors":[{"name":"Jakub T. Jankiewicz","title":"LIPS maintainer","url":"https://jakub.jankiewicz.org/","imageURL":"https://github.com/jcubic.png","key":"jcubic","page":null}],"frontMatter":{"slug":"lips-history","title":"LIPS Scheme History","authors":"jcubic","image":"/img/lips-history.png","tags":["lips","scheme","history"]},"unlisted":false,"prevItem":{"title":"Scheme Regex literals in Emacs","permalink":"/blog/emacs-scheme-regex"}},"content":"This is the first article on LIPS blog. In this article I will write about the history of LIPS\\nScheme interpreter.\\n\\n\x3c!--truncate--\x3e\\n\\n## What is Scheme?\\n\\nScheme is a dialect of the Lisp. The second oldest programming language still in use (after\\nFortran).  Lisp and Scheme have specific syntax with prefix notation and where everything is a list\\n(at least historically).  It\'s also [Homoiconic](https://en.wikipedia.org/wiki/Homoiconicity), which\\nmeans that code and data have the same represantion. This allows to write programs that modify the\\ncode like it was data.\\n\\n## What is LIPS?\\n\\nLIPS name is a recursive ancronym which stands for **\\"LIPS Is Pretty Simple\\"**. LIPS Scheme is\\nimplementation of Scheme programming language in JavaScript. It adds a lot of stuff on top of Scheme\\nto make it more powerful and easier to interact with JavaScript.\\n\\n## History of LIPS\\n\\nIt all started in February 2018 when I\'ve written the first version of a Lisp interpreter. You can\\nstill see the code on [CodePen](https://codepen.io/jcubic/pen/gvvzdp). Then I moved the [development\\nto GitHub](https://github.com/jcubic/lips) and named the project LIPS.  The first release (version\\n0.2.0) is marked as Mar 2018.\\n\\nThe reason why I created another lisp in JavaScript was because I wanted to have an Emacs in browser\\nthat would have a real lisp inside. That\'s why LIPS had dynamic scope as an option. GNU Emacs use\\nElisp that for a long time had dynamic scope. So I was planing to emulate that.\\n\\nAt the beginning it was Lisp based on Scheme, but at one point after version\\n[0.20.1 dated as Jul 1, 2020](https://github.com/jcubic/lips/releases/tag/0.20.1), I\'ve started\\nadding features on devel branch and decided that I want a full Scheme implementation. But it turns out\\nthat there were way too many breaking changes to release the next version. So I decided that I will\\nrelease it as 1.0-beta. Since then, LIPS keeps introducing new Beta versions. You can see the\\n[latest release on GitHub](https://github.com/jcubic/lips/releases).\\n\\n## Future of LIPS\\n\\nFor the future plans I want in final version 1.0 are implementation of continutations and Tail Calls\\n(<abbr title=\\"Tail Call Optimization\\">TCO</abbr>) and to be compatible (more or less) with\\n[R<sup>7</sup>RS specification](https://standards.scheme.org/). To see the progress, you can check\\n[1.0 Milestone on GitHub](https://github.com/jcubic/lips/issues?q=is%3Aopen+is%3Aissue+milestone%3A1.0)."}]}}')}}]);